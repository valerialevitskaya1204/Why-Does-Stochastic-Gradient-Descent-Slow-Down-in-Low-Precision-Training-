# Why-Does-Stochastic-Gradient-Descent-Slow-Down-in-Low-Precision-Training-

This repository contains code and experiments for reproducing and analyzing the slowdown effects in low-precision neural network training, based on the paper [Why Does Stochastic Gradient Descent Slow Down in Low-Precision Training?](https://openreview.net/forum?id=ZXkC0aYMDa).

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ main.py                      #to run all experiments
â”œâ”€â”€ algorithms/
â”‚   â”œâ”€â”€ configs.py               #all configurations
â”‚   â””â”€â”€ optim.py                 # SAGA, SAG, Adam updates
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ data/                    #store datasets
â”‚   â”œâ”€â”€ data/load_everything.py  #load datasets (CIFAR10 and mushrooms)
â”‚   â”œâ”€â”€ plots/plot_smth.py       #to plot
â”‚   â”œâ”€â”€ metrics.py               #logloss and accuracy
â”‚   â”œâ”€â”€ models.py                #modified ResNet50 for baseline
â”‚   â””â”€â”€ train.py                 #train different experiments
â”œâ”€â”€ quantization/
â”‚   â””â”€â”€ quant.py                 #quantization for different precisions
â””â”€â”€ figures/                     #plots and results
```

## Overview

This project investigates the convergence slowdown in low-precision (FP4) neural network training across different optimization algorithms (SAGA, SAG, Adam).

### Contribution

1. **ResNet50 on CIFAR10**:
   - reproduced the original paper's setup
   - observed convergence slowdown at 60-100 epochs for FP4 with LR=1e-4

2. **Optimization Algorithms Analysis**:
   - Used logistic regression on mushrooms dataset to avoid memory constraints
   - Tested Adam, SAGA, and SAG optimizers
   - Consistently observed slowdown in FP4 precision with LR=1e-4

3. **SAGA, SAG, Adam**:
   - mean magnitude of shrinkage factor (q = â€–gÌƒâ€–â‚‚/â€–gâ€–â‚‚)
   - quantization maps small gradient values to zero/coarse levels, reducing overall magnitude
   - FP4 shows noticeable decrease in shrinkage

## ğŸ“Š Experimental Results

### Baseline Performance
![Baseline for ResNet50](figures/base.png)
*Figure 1: Baseline convergence for ResNet50 on CIFAR10*

### SGD Slowdown
![SGD slowdown](figures/60_100.png)
*Figure 2: SGD slowdown observed at 60-100 epochs with FP4 precision*

### Adam Optimization
![Adam slowdown](figures/adam.png)
*Figure 3: Convergence slowdown for Adam optimizer with FP4 precision*

### SAG Optimization
![SAG slowdown](figures/sag.png)
*Figure 4: Convergence slowdown for SAG optimizer with FP4 precision*

### SAGA Optimization
![SAGA slowdown](figures/saga.png)
*Figure 5: SAGA slowdown with decreasing shrinkage factor in FP4 precision*

## How to use

1. **Install dependencies**:
```bash
pip install -r requirements.txt
```

2. **Run all experiments**:
```bash
python main.py
```

3. **Generate plots**:
```bash
python training/plots/plot_smth.py
```
